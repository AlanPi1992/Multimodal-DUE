{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = os.path.join('..', '..', 'data', 'input_files_multimodal_SR41', 'single_class_input_files')\n",
    "output_directory = os.path.join('..', '..', 'data', 'input_files_multimodal_SR41')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read transit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name_transit = os.path.join(output_directory, 'transit_route_stops')\n",
    "bus_route_dic = {} # route_name : {'frequency(s)': int, 'DMOND_ID': str, 'DMDND_ID': str, 'stops':[list of node ID]}\n",
    "with open(input_file_name_transit, 'r') as infile:\n",
    "    for line in infile.readlines():\n",
    "        if line[0] != '#':\n",
    "            data = line.rstrip('\\n').split(' ')\n",
    "#             print data\n",
    "            bus_route_dic[data[0]] = {}\n",
    "            bus_route_dic[data[0]]['frequency'] =  int(data[1])\n",
    "            bus_route_dic[data[0]]['DMOND_ID'] =  data[2]\n",
    "            bus_route_dic[data[0]]['DMDND_ID'] =  data[3]\n",
    "            bus_route_dic[data[0]]['stops'] =  data[4:]\n",
    "            \n",
    "print bus_route_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read parking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'24': {'avg_crusing_time': 100, 'connections': ['1133'], 'fee': 10, 'distances': [100], 'total_capacity': 1000}, '25': {'avg_crusing_time': 100, 'connections': ['1184'], 'fee': 10, 'distances': [100], 'total_capacity': 1000}, '20': {'avg_crusing_time': 100, 'connections': ['1138', '1127'], 'fee': 10, 'distances': [200, 200], 'total_capacity': 1000}, '21': {'avg_crusing_time': 100, 'connections': ['1203'], 'fee': 10, 'distances': [100], 'total_capacity': 1000}, '22': {'avg_crusing_time': 100, 'connections': ['1135', '1147'], 'fee': 10, 'distances': [200, 200], 'total_capacity': 1000}, '23': {'avg_crusing_time': 100, 'connections': ['1213'], 'fee': 10, 'distances': [100], 'total_capacity': 1000}, '1': {'avg_crusing_time': 20, 'connections': ['236'], 'fee': 0, 'distances': [100], 'total_capacity': 2000}, '3': {'avg_crusing_time': 20, 'connections': ['794', '771'], 'fee': 0, 'distances': [200, 200], 'total_capacity': 2000}, '2': {'avg_crusing_time': 20, 'connections': ['214', '759'], 'fee': 0, 'distances': [200, 200], 'total_capacity': 2000}, '5': {'avg_crusing_time': 30, 'connections': ['135', '134'], 'fee': 1, 'distances': [200, 200], 'total_capacity': 2000}, '4': {'avg_crusing_time': 20, 'connections': ['796', '782'], 'fee': 0, 'distances': [200, 200], 'total_capacity': 2000}, '7': {'avg_crusing_time': 30, 'connections': ['227', '18'], 'fee': 1, 'distances': [200, 200], 'total_capacity': 2000}, '6': {'avg_crusing_time': 30, 'connections': ['832', '848'], 'fee': 1, 'distances': [200, 200], 'total_capacity': 2000}, '9': {'avg_crusing_time': 40, 'connections': ['533', '240'], 'fee': 2, 'distances': [200, 200], 'total_capacity': 2000}, '8': {'avg_crusing_time': 40, 'connections': ['865', '872'], 'fee': 2, 'distances': [200, 200], 'total_capacity': 2000}, '11': {'avg_crusing_time': 60, 'connections': ['232', '515'], 'fee': 3, 'distances': [200, 200], 'total_capacity': 1500}, '10': {'avg_crusing_time': 40, 'connections': ['921', '926'], 'fee': 2, 'distances': [200, 200], 'total_capacity': 2000}, '13': {'avg_crusing_time': 60, 'connections': ['496', '961'], 'fee': 3, 'distances': [200, 200], 'total_capacity': 1500}, '12': {'avg_crusing_time': 60, 'connections': ['955', '962'], 'fee': 3, 'distances': [200, 200], 'total_capacity': 1500}, '15': {'avg_crusing_time': 60, 'connections': ['1003', '501'], 'fee': 4, 'distances': [200, 200], 'total_capacity': 1500}, '14': {'avg_crusing_time': 60, 'connections': ['1008', '1010'], 'fee': 3, 'distances': [200, 200], 'total_capacity': 1500}, '17': {'avg_crusing_time': 60, 'connections': ['30'], 'fee': 5, 'distances': [100], 'total_capacity': 1500}, '16': {'avg_crusing_time': 60, 'connections': ['219', '1050'], 'fee': 5, 'distances': [200, 200], 'total_capacity': 1500}, '19': {'avg_crusing_time': 60, 'connections': ['1201'], 'fee': 5, 'distances': [100], 'total_capacity': 1500}, '18': {'avg_crusing_time': 60, 'connections': ['1087', '1090'], 'fee': 5, 'distances': [200, 200], 'total_capacity': 1500}}\n"
     ]
    }
   ],
   "source": [
    "input_file_name_parking = os.path.join(output_directory, 'parking_lot')\n",
    "parking_dic = {} # parking_lot_ID : {'fee($)': int, 'total_capacity': str, 'avg_crusing_time(s)': str, \n",
    "                                   # 'connections':[node ID], 'distances':[meters]}\n",
    "with open(input_file_name_parking, 'r') as infile:\n",
    "    for line in infile.readlines():\n",
    "        if line[0] != '#':\n",
    "            data = line.rstrip('\\n').split(' ')\n",
    "            parking_dic[data[0]] = {}\n",
    "            parking_dic[data[0]]['fee'] =  int(data[1])\n",
    "            parking_dic[data[0]]['total_capacity'] =  int(data[2])\n",
    "            parking_dic[data[0]]['avg_crusing_time'] =  int(data[3])\n",
    "            parking_dic[data[0]]['connections'] = []\n",
    "            parking_dic[data[0]]['distances'] = []\n",
    "            for i in range(4, len(data)):\n",
    "                parking_dic[data[0]]['connections'].append(data[i].split(':')[0])\n",
    "                parking_dic[data[0]]['distances'].append(int(data[i].split(':')[1]))\n",
    "print parking_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name_node = os.path.join(input_directory, 'MNM_input_node')\n",
    "output_file_name_node = os.path.join(output_directory, 'MNM_input_node')\n",
    "node_str = '# ID Type Convert_factor\\n'\n",
    "\n",
    "with open(input_file_name_node, 'r') as infile:\n",
    "    for line in infile.readlines():\n",
    "        if line[0] != '#':\n",
    "            data = line.rstrip('\\r\\n').split(' ')\n",
    "            data.append('2.0')\n",
    "            node_str += ' '.join(data) + '\\n'\n",
    "    for parking_id in parking_dic:\n",
    "        new_node_ID = str(90000 + int(parking_id))\n",
    "        node_str += ' '.join([new_node_ID, 'DMDND', '2.0']) + '\\n'\n",
    "        \n",
    "f = open(output_file_name_node, 'w')\n",
    "f.write(node_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process OD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_mapping = {} # node_id : OD_id\n",
    "input_file_name_od = os.path.join(input_directory, 'MNM_input_od')\n",
    "output_file_name_od = os.path.join(output_directory, 'MNM_input_od')\n",
    "od_str = ''\n",
    "\n",
    "with open(input_file_name_od, 'r') as infile:\n",
    "    for line in infile.readlines():\n",
    "        od_str += line\n",
    "        if line[0] != '#':\n",
    "            data = line.rstrip('\\r\\n').split(' ')\n",
    "            OD_mapping[data[1]] = data[0]\n",
    "            \n",
    "    for parking_id in parking_dic:\n",
    "        new_node_ID = str(90000 + int(parking_id))\n",
    "        od_str += ' '.join([new_node_ID, new_node_ID]) + '\\n'\n",
    "        \n",
    "f = open(output_file_name_od, 'w')\n",
    "f.write(od_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Snap_graph & links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name_graph = os.path.join(input_directory, 'Snap_graph')\n",
    "output_file_name_graph = os.path.join(output_directory, 'Snap_graph')\n",
    "graph_str = ''\n",
    "\n",
    "input_file_name_link = os.path.join(input_directory, 'MNM_input_link')\n",
    "output_file_name_link = os.path.join(output_directory, 'MNM_input_link')\n",
    "link_str = '# ID Type LEN(mile) FFS(mile/h) Cap(v/hour) RHOJ(v/miles) Lane FFS_truck(mile/h) Cap_truck(v/hour) RHOJ_truck(v/miles) Convert_factor(1)\\n'\n",
    "\n",
    "_count_new_link = 0\n",
    "with open(input_file_name_graph, 'r') as infile:\n",
    "    with open(input_file_name_link, 'r') as infile2:\n",
    "        for line in infile.readlines():\n",
    "            graph_str += line\n",
    "        for line in infile2.readlines():\n",
    "            if line[0] != '#':\n",
    "                data = line.rstrip('\\r\\n').split(' ')\n",
    "                if data[1] == 'PQ':\n",
    "                    data += [data[3], data[4], data[5], '2.0']\n",
    "                else:\n",
    "                    data += [str(float(data[3])*0.9), str(float(data[4])*0.9), str(float(data[5])*0.8), '2.0']\n",
    "                newline = ' '.join(data) + '\\n'\n",
    "                link_str += newline\n",
    "        \n",
    "        \n",
    "        for parking_id in parking_dic:\n",
    "            to_node_ID = str(90000 + int(parking_id))\n",
    "            for node in parking_dic[parking_id]['connections']:\n",
    "                _count_new_link += 1\n",
    "                link_ID = str(1000000 + _count)\n",
    "                link_str += ' '.join([link_ID, 'PQ', '1', '99999', '99999', '99999', '1', \n",
    "                                      '99999', '99999', '99999', '2.0']) + '\\n'\n",
    "                from_node_ID = node\n",
    "                graph_str += ' '.join([link_ID, from_node_ID, to_node_ID]) + '\\n'\n",
    "        \n",
    "f = open(output_file_name_graph, 'w')\n",
    "f.write(graph_str)\n",
    "f.close()\n",
    "\n",
    "f = open(output_file_name_link, 'w')\n",
    "f.write(link_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "print len(parking_dic)\n",
    "print _count_new_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "[4259.979539999951, 4259.979539999951, 12779.938619999946, 12779.938619999946, 21299.89770000001, 21299.89770000001, 21299.89770000001, 21299.89770000001, 12779.938619999946, 12779.938619999946, 4259.979539999951, 4259.979539999951, 473.3310600000054, 473.3310600000054, 1419.9931800000268, 1419.9931800000268, 2366.6553000000135, 2366.6553000000135, 2366.6553000000135, 2366.6553000000135, 1419.9931800000268, 1419.9931800000268, 473.3310600000054, 473.3310600000054]\n",
      "170399.1816\n"
     ]
    }
   ],
   "source": [
    "input_file_name_demand = os.path.join(input_directory, 'MNM_input_demand')\n",
    "output_file_name_demand = os.path.join(output_directory, 'MNM_input_demand')\n",
    "demand_str = '# Origin_ID Destination_ID <demand by interval> <truck demand by interval>\\n'\n",
    "\n",
    "with open(input_file_name_demand, 'r') as infile:\n",
    "    for line in infile.readlines():\n",
    "        if line[0] != '#':\n",
    "            demands = line.rstrip('\\n').split(' ')\n",
    "            num_int = len(demands) - 2\n",
    "            int_sums = [0 for i in range(num_int*2)]\n",
    "            break\n",
    "print num_int\n",
    "\n",
    "with open(input_file_name_demand, 'r') as infile:\n",
    "    for line in infile.readlines():\n",
    "        if line[0] != '#':\n",
    "            demands = line.rstrip('\\n').split(' ')\n",
    "            for i in range(2, 2+num_int):\n",
    "                demands.append(str(float(demands[i]) * 0.1))\n",
    "            for i in range(2, 2+num_int):\n",
    "                demands[i] = str(float(demands[i]) * 0.9)\n",
    "            demand_str += ' '.join(demands) + '\\n'\n",
    "            \n",
    "            for i in range(num_int):\n",
    "                int_sums[i] += float(demands[2 + i])\n",
    "                int_sums[i + num_int] += float(demands[2 + i + num_int])\n",
    "                \n",
    "print int_sums \n",
    "print sum(int_sums) \n",
    "\n",
    "f = open(output_file_name_demand, 'w')\n",
    "f.write(demand_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
